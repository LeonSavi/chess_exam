---
language: en
tags:
- chess
- transformer
- pytorch
license: mit
---

# Chess-God-Transformer ‚ôüÔ∏èü§ñ
This model is a custom-built Encoder-Decoder Transformer designed to predict the optimal next move in a chess game given a Board State in FEN (Forsyth-Edwards Notation).

## Context
This project was developed for my transformer class. I decided to challenge myself, rather than training an existing model I built it from scratch. **terrible decision**...but here we are.

## Data
To prevent common "hallucinations", I implemented a multi-stage data curriculum:
1.  **Stockfish-GM Seed:** ~13,000 high-accuracy moves generated by Stockfish 16 at Grandmaster and Strong level. 
2.  **Tactics Injection:** ~2,000,000+ positions from the `ssingh22/chess-evaluations` (tactics subset).
3.  **Human Context:** Integrated 30,000 moves from `bonna46/Chess-FEN-and-NL-Format`.
4.  **Deduplication:** Strategic filtering was applied to the starting 15 plies to prevent the model from overfitting to common openings.

## Technical Architecture
The core architecture is based on the original "Attention is All You Need" paper, specifically following the implementation guide from [DataCamp's Transformer Tutorial](https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch).

**Hyperparameters Tuning**
Optuna was used for fine tuning the hyperparameters. I run 70 trials with 15 epochs each. We sampled 10% the training data, and used 80% for training and 20% for validation. The search algorithm, it is in similar fashion another [project](https://github.com/LeonSavi/ADS-DW3) I did, and it focused on minimizing two factors:
1. CrossEntropyLoss
2. CrossEntropyLoss Gap between Training Set and Validation Set. This a way to minimize overfitting.

The following are the resulting hyperparameter

```yml
d_model: 128      
num_heads: 8      
num_layers: 4     
d_ff: 512         # 4 x d_model
dropout: 0.1     
lr: 0.0003        
batch_size: 128   
```

## Training
Once the optimal Hyperparameters have been found, we run the trainer. We run 100 epochs and trains on 80% of the whole shuffled data (~2,000,000) while the ramaining are used for validation. the model **TransformerGodPlayer.pth** is saved in model folder and uploaded in HuggingFace along with **opt-configs.yml**. 


## How to Use
The model automatically imports a ad-hoc `ChessTokenizer` and the `Transformer` class to load the `.pth` - weights, first attemps to load those locally (since the GitHub repo is going to be cloned), otherwise import from HuggingFace.

```python
from player import TransformerPlayer

model = TransformerPlayer() #everything is already initialized

fen = "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
move = model.get_move(fen) 
print(f"God-Transformer predicts: {move}")
```

**Fallback**
As a probabilistic model, this Transformer occasionally predicts illegal moves. I have implemented a python-chess validation layer in get_move() to ensure all tournament moves are legal. I attempt my luck with a random legal move.


