---
language: en
tags:
- chess
- transformer
- pytorch
license: mit
---

# Chess-God-Transformer ‚ôüÔ∏èü§ñ

This model is a custom-built Encoder-Decoder Transformer designed to predict the optimal next move in a chess game given a Board State in FEN (Forsyth-Edwards Notation).

## üéì Academic Context
This project was developed as part of a Chess AI tournament. The goal was to move beyond simple heuristic-based bots and create a generative model capable of "understanding" chess positioning through self-attention.

## üèóÔ∏è Technical Architecture
The core architecture is based on the original "Attention is All You Need" paper, specifically following the implementation guide from [DataCamp's Transformer Tutorial](https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch).

**Key Scaling & Modifications:**
While the base logic follows the tutorial, the following enhancements were made to handle competitive play:
* **Dimensionality:** Scaled `d_model` to 128 and `d_ff` to 512/1024 to capture complex tactical patterns.
* **Depth:** Increased to 4-6 layers to allow for deeper relational reasoning between pieces.
* **Tokenization:** A custom character-level tokenizer was developed to handle the 33 unique characters found in FEN and UCI move notation.

## üìä The "God-Mode" Data Strategy
To prevent common "hallucinations", I implemented a multi-stage data curriculum:
1.  **Stockfish-GM Seed:** ~13,000 high-accuracy moves generated by Stockfish 16 at Grandmaster and Strong level .
2.  **Tactics Injection:** ~2,000,000+ positions from the `ssingh22/chess-evaluations` (tactics subset).
3.  **Human Context:** Integrated 30,000 moves from `bonna46/Chess-FEN-and-NL-Format`.
4.  **Deduplication:** Strategic filtering was applied to the starting 15 plies to prevent the model from overfitting to common openings.

## How to Use
The model automatically imports a custom `ChessTokenizer` and the `Transformer` class to load the `.pth` - weights, first attemps to load those locally (since the GitHub repo is going to be cloned),  otherwise import from HuggingFace.

```python
from player import TransformerPlayer

model = TransformerPlayer() #everything is already initialized

fen = "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"
move = model.get_move(fen) 
print(f"God-Transformer predicts: {move}")
```